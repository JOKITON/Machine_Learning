{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "# from conc_obj import EEGData\n",
    "from eegdata_multi import EEGData\n",
    "from plt import plot_psd, plot_montage\n",
    "from ica import plot_ica_comp\n",
    "\n",
    "# MNE imports\n",
    "import mne\n",
    "from mne.io.edf import read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Macros***\n",
    "\n",
    ">General use macros, importing JSON files to use as the configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nT0 corresponds to rest\\nT1 corresponds to onset of motion (real or imagined) of\\nthe left fist (in runs 3, 4, 7, 8, 11, and 12)\\nboth fists (in runs 5, 6, 9, 10, 13, and 14)\\nT2 corresponds to onset of motion (real or imagined) of\\nthe right fist (in runs 3, 4, 7, 8, 11, and 12)\\nboth feet (in runs 5, 6, 9, 10, 13, and 14)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_path = Path().resolve()\n",
    "folder = (script_path / \"../\").resolve()\n",
    "\n",
    "JSON_MAIN_PATH = script_path / \"config_main.json\"\n",
    "JSON_CSP_PATH = script_path / \"config_csp.json\"\n",
    "EVENTS_PATH = script_path / \"events.json\"\n",
    "\n",
    "with open(JSON_MAIN_PATH, \"r\") as f:\n",
    "    config_main = json.load(f)\n",
    "\n",
    "with open(JSON_CSP_PATH, \"r\") as f:\n",
    "    config_csp = json.load(f)\n",
    "\n",
    "VERBOSE = config_main['verbose'].lower() == 'true'\n",
    "\n",
    "L_FREQ = config_main['l_freq']\n",
    "H_FREQ = config_main['h_freq']\n",
    "\n",
    "N_SUBJECTS = config_main[\"n_subjects\"]\n",
    "N_COMPONENTS = config_main[\"n_components\"]\n",
    "\n",
    "\"\"\"\n",
    "T0 corresponds to rest\n",
    "T1 corresponds to onset of motion (real or imagined) of\n",
    "the left fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "both fists (in runs 5, 6, 9, 10, 13, and 14)\n",
    "T2 corresponds to onset of motion (real or imagined) of\n",
    "the right fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "both feet (in runs 5, 6, 9, 10, 13, and 14)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialization of EEG object***\n",
    "\n",
    ">***(If the files are not locally stored, it will download them to the user system automatically)***\n",
    "\n",
    ">***Use of functions like .filter_data() also is obligatory if there is no data stored locally***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data:\n",
      "<Raw | sample_mmi_h_raw.fif, 64 x 2333120 (14582.0 s), ~1.11 GiB, data loaded> <Raw | sample_mmi_hf_raw.fif, 64 x 2333120 (14582.0 s), ~1.11 GiB, data loaded>\n",
      "<Raw | sample_mmi_h_filt_raw.fif, 64 x 2333120 (14582.0 s), ~1.11 GiB, data loaded> <Raw | sample_mmi_hf_filt_raw.fif, 64 x 2333120 (14582.0 s), ~1.11 GiB, data loaded>\n"
     ]
    }
   ],
   "source": [
    "eeg_obj = EEGData(config_main, config_csp, folder, verbose=VERBOSE)\n",
    "# eeg_obj.save_type_data(type=\"events\", folder_path=folder, verbose=VERBOSE)\n",
    "\n",
    "#* Filters data and plots PSD to see differences\n",
    "# eeg_obj.filter_data()\n",
    "# eeg_obj.plot_psd_ba_filt(verbose=VERBOSE)\n",
    "\n",
    "# eeg_obj.plot_psd(verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic information and montage plotting in 2D & 3D**\n",
    "> ***The channel names can also be printed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* Plots different montages in 2D & 3D\n",
    "# data = eeg_obj.get_raw_h()\n",
    "# ch_names = data.info[\"ch_names\"] \n",
    "# plot_montage(eeg_obj.montage, ch_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ICA(Independent Component Analysys)**\n",
    "> ***The number of components that ICA will try to sort out can be changed, it is advised to use values in the range [15-45]***\n",
    "\n",
    "> ***Ocular artifacts are also removed, since they don't contribute to the muscular movement on this evaluation***\n",
    "\n",
    "> ***The components can also be plotted and ocular artifacts, EOG, will be clearly visible***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before :  [[2, 4]]\n",
      "After :  {'do/right_hand': 2, 'imagine/right_hand': 4}\n",
      "Using data from preloaded Raw for 872 events and 433 original time points ...\n",
      "0 bad epochs dropped\n",
      "(872, 64, 433)\n",
      "(872,)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00085 (2.2e-16 eps * 64 dim * 5.9e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=2 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=4 covariance using EMPIRICAL\n",
      "Done.\n",
      "(872, 64, 433)\n",
      "(872,)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00085 (2.2e-16 eps * 64 dim * 5.9e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=2 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=4 covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shared/42/ML/.venv/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:595: UserWarning: n_components is too large: it will be set to 16\n",
      "  warnings.warn(\n",
      "/Users/Shared/42/ML/.venv/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy LDA: 0.6413199426111909\n",
      "Train Accuracy SVM: 1.0\n",
      "Train Accuracy RF: 1.0\n",
      "LDA Accuracy: 0.56\n",
      "SVM Accuracy: 0.8914285714285715\n",
      "Random Forest Accuracy: 0.9314285714285714\n",
      "Confusion Matrix LDA:\n",
      " [[39 48]\n",
      " [29 59]]\n",
      "Confusion Matrix SVM:\n",
      " [[84  3]\n",
      " [16 72]]\n",
      "Confusion Matrix RF:\n",
      " [[81  6]\n",
      " [ 6 82]]\n"
     ]
    }
   ],
   "source": [
    "ev_list = config_csp[\"ev_mlist_three\"]\n",
    "\n",
    "data_h, data_hf = eeg_obj.get_filt()\n",
    "events_h, events_hf = eeg_obj.get_events()\n",
    "\n",
    "epochs, freq_bands = eeg_obj.crt_epochs(data_h, events_h, ev_list, \"hands\", verbose=VERBOSE)\n",
    "\n",
    "features, labels = eeg_obj.simple_csp(epochs, freq_bands, verbose=VERBOSE)\n",
    "\n",
    "#* Computes ICA components\n",
    "# eeg_obj.decomp_ica(n_components=N_COMPONENTS, plt_show=True, verbose=VERBOSE)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "from sklearn.decomposition import FastICA\n",
    "features = FastICA(n_components=N_COMPONENTS).fit_transform(features)\n",
    "\n",
    "#* Plot components of ICA\n",
    "# plot_ica_comp(folder / config[\"path_ica_h\"])\n",
    "\n",
    "#* Trains and evaluates model\n",
    "\n",
    "#* Normalizes data\n",
    "features = eeg_obj.normalize_data(features)\n",
    "   \n",
    "eeg_obj.train_model(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Saves filtered and concatenated data for faster loading\n",
    "# eeg_obj.save_type_data(type=\"raw\", folder_path=folder)\n",
    "# eeg_obj.save_type_data(type=\"filtered\", folder_path=folder)\n",
    "eeg_obj.save_type_data(type=\"epochs\", folder_path=folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
